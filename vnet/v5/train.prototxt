layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  python_param {
    module: "VnetDataLayer"
    layer: "VnetDataLayer"
    param_str: "{\'vol_size\': 64, \'iter_count\': 100000, \'rotate_ratio\': 0.9, \'queue_size\': 30, \'phase\': \'train\', \'data_path\': \'d:/project/tianchi/data/\', \'shift_ratio\': 0.9, \'histogram_shift_ratio\': 0.9, \'net_path\': \'v5/\', \'batch_size\': 2}"
  }
}

# input

layer {
  name: "conv_input_split"
  type: "Convolution"
  bottom: "data"
  top: "conv_input_split"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 16
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "constant" value: 1.0 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

# 64 - stage a

layer {
  name: "conv_input_64_a1"
  type: "Convolution"
  bottom: "conv_input_split"
  top: "conv_input_64_a1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 8
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_64_a1"
  type: "BatchNorm"
  bottom: "conv_input_64_a1"
  top: "conv_input_64_a1"
}

layer {
  name: "relu_input_64_a1"
  type: "PReLU"
  bottom: "conv_input_64_a1"
  top: "conv_input_64_a1"
}

layer {
  name: "conv_input_64_a2"
  type: "Convolution"
  bottom: "conv_input_64_a1"
  top: "conv_input_64_a2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 8
    kernel_size: 3
    pad: 1
    stride: 1
    group: 2
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_64_a2"
  type: "BatchNorm"
  bottom: "conv_input_64_a2"
  top: "conv_input_64_a2"
}

layer {
  name: "relu_input_64_a2"
  type: "PReLU"
  bottom: "conv_input_64_a2"
  top: "conv_input_64_a2"
}

layer {
  name: "conv_input_64_a3"
  type: "Convolution"
  bottom: "conv_input_64_a2"
  top: "conv_input_64_a3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 16
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_64_a3"
  type: "BatchNorm"
  bottom: "conv_input_64_a3"
  top: "conv_input_64_a3"
}

layer {
  name: "add_input_64_a"
  type: "Eltwise"
  bottom: "conv_input_split"
  bottom: "conv_input_64_a3"
  top: "conv_input_64_a3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_add_input_64_a"
  type: "PReLU"
  bottom: "conv_input_64_a3"
  top: "conv_input_64_a3"
}

# pooling
layer {
  name: "conv_pooling_i64c128o32"
  type: "Convolution"
  bottom: "conv_input_64_a3"
  top: "conv_pooling_i64c128o32"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 2
    pad: 0
    stride: 2
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

# 32 - stage a

layer {
  name: "conv_input_32_a1"
  type: "Convolution"
  bottom: "conv_pooling_i64c128o32"
  top: "conv_input_32_a1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 64
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_32_a1"
  type: "BatchNorm"
  bottom: "conv_input_32_a1"
  top: "conv_input_32_a1"
}

layer {
  name: "relu_input_32_a1"
  type: "PReLU"
  bottom: "conv_input_32_a1"
  top: "conv_input_32_a1"
}

layer {
  name: "conv_input_32_a2"
  type: "Convolution"
  bottom: "conv_input_32_a1"
  top: "conv_input_32_a2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 64
    kernel_size: 3
    pad: 1
    stride: 1
    group: 16
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_32_a2"
  type: "BatchNorm"
  bottom: "conv_input_32_a2"
  top: "conv_input_32_a2"
}

layer {
  name: "relu_input_32_a2"
  type: "PReLU"
  bottom: "conv_input_32_a2"
  top: "conv_input_32_a2"
}

layer {
  name: "conv_input_32_a3"
  type: "Convolution"
  bottom: "conv_input_32_a2"
  top: "conv_input_32_a3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_32_a3"
  type: "BatchNorm"
  bottom: "conv_input_32_a3"
  top: "conv_input_32_a3"
}

layer {
  name: "add_input_32_a"
  type: "Eltwise"
  bottom: "conv_pooling_i64c128o32"
  bottom: "conv_input_32_a3"
  top: "conv_input_32_a3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_add_input_32_a"
  type: "PReLU"
  bottom: "conv_input_32_a3"
  top: "conv_input_32_a3"
}

# 32 - stage b

layer {
  name: "conv_input_32_b1"
  type: "Convolution"
  bottom: "conv_input_32_a3"
  top: "conv_input_32_b1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 64
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_32_b1"
  type: "BatchNorm"
  bottom: "conv_input_32_b1"
  top: "conv_input_32_b1"
}

layer {
  name: "relu_input_32_b1"
  type: "PReLU"
  bottom: "conv_input_32_b1"
  top: "conv_input_32_b1"
}

layer {
  name: "conv_input_32_b2"
  type: "Convolution"
  bottom: "conv_input_32_b1"
  top: "conv_input_32_b2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 64
    kernel_size: 3
    pad: 1
    stride: 1
    group: 16
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_32_b2"
  type: "BatchNorm"
  bottom: "conv_input_32_b2"
  top: "conv_input_32_b2"
}

layer {
  name: "relu_input_32_b2"
  type: "PReLU"
  bottom: "conv_input_32_b2"
  top: "conv_input_32_b2"
}

layer {
  name: "conv_input_32_b3"
  type: "Convolution"
  bottom: "conv_input_32_b2"
  top: "conv_input_32_b3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_input_32_b3"
  type: "BatchNorm"
  bottom: "conv_input_32_b3"
  top: "conv_input_32_b3"
}

layer {
  name: "add_input_32_b"
  type: "Eltwise"
  bottom: "conv_input_32_a3"
  bottom: "conv_input_32_b3"
  top: "conv_input_32_b3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_bdd_input_32_b"
  type: "PReLU"
  bottom: "conv_input_32_b3"
  top: "conv_input_32_b3"
}

# pooling

layer {
  name: "conv_pooling_i32c256o16"
  type: "Convolution"
  bottom: "conv_input_32_b3"
  top: "conv_pooling_i32c256o16"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 256
    kernel_size: 2
    pad: 0
    stride: 2
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

# vally

# stage a

layer {
  name: "conv_vally_a1"
  type: "Convolution"
  bottom: "conv_pooling_i32c256o16"
  top: "conv_vally_a1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_a1"
  type: "BatchNorm"
  bottom: "conv_vally_a1"
  top: "conv_vally_a1"
}

layer {
  name: "relu_vally_a1"
  type: "PReLU"
  bottom: "conv_vally_a1"
  top: "conv_vally_a1"
}

layer {
  name: "conv_vally_a2"
  type: "Convolution"
  bottom: "conv_vally_a1"
  top: "conv_vally_a2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 3
    pad: 1
    stride: 1
    group: 32
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_a2"
  type: "BatchNorm"
  bottom: "conv_vally_a2"
  top: "conv_vally_a2"
}

layer {
  name: "relu_vally_a2"
  type: "PReLU"
  bottom: "conv_vally_a2"
  top: "conv_vally_a2"
}

layer {
  name: "conv_vally_a3"
  type: "Convolution"
  bottom: "conv_vally_a2"
  top: "conv_vally_a3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 256
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_a3"
  type: "BatchNorm"
  bottom: "conv_vally_a3"
  top: "conv_vally_a3"
}

layer {
  name: "add_vally_a"
  type: "Eltwise"
  bottom: "conv_pooling_i32c256o16"
  bottom: "conv_vally_a3"
  top: "conv_vally_a3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_add_vally_a"
  type: "PReLU"
  bottom: "conv_vally_a3"
  top: "conv_vally_a3"
}

# stage b

layer {
  name: "conv_vally_b1"
  type: "Convolution"
  bottom: "conv_vally_a3"
  top: "conv_vally_b1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_b1"
  type: "BatchNorm"
  bottom: "conv_vally_b1"
  top: "conv_vally_b1"
}

layer {
  name: "relu_vally_b1"
  type: "PReLU"
  bottom: "conv_vally_b1"
  top: "conv_vally_b1"
}

layer {
  name: "conv_vally_b2"
  type: "Convolution"
  bottom: "conv_vally_b1"
  top: "conv_vally_b2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 3
    pad: 1
    stride: 1
    group: 32
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_b2"
  type: "BatchNorm"
  bottom: "conv_vally_b2"
  top: "conv_vally_b2"
}

layer {
  name: "relu_vally_b2"
  type: "PReLU"
  bottom: "conv_vally_b2"
  top: "conv_vally_b2"
}

layer {
  name: "conv_vally_b3"
  type: "Convolution"
  bottom: "conv_vally_b2"
  top: "conv_vally_b3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 256
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_b3"
  type: "BatchNorm"
  bottom: "conv_vally_b3"
  top: "conv_vally_b3"
}

layer {
  name: "add_vally_b"
  type: "Eltwise"
  bottom: "conv_vally_a3"
  bottom: "conv_vally_b3"
  top: "conv_vally_b3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_add_vally_b"
  type: "PReLU"
  bottom: "conv_vally_b3"
  top: "conv_vally_b3"
}

# stage c

layer {
  name: "conv_vally_c1"
  type: "Convolution"
  bottom: "conv_vally_b3"
  top: "conv_vally_c1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_c1"
  type: "BatchNorm"
  bottom: "conv_vally_c1"
  top: "conv_vally_c1"
}

layer {
  name: "relu_vally_c1"
  type: "PReLU"
  bottom: "conv_vally_c1"
  top: "conv_vally_c1"
}

layer {
  name: "conv_vally_c2"
  type: "Convolution"
  bottom: "conv_vally_c1"
  top: "conv_vally_c2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 3
    pad: 1
    stride: 1
    group: 32
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_c2"
  type: "BatchNorm"
  bottom: "conv_vally_c2"
  top: "conv_vally_c2"
}

layer {
  name: "relu_vally_c2"
  type: "PReLU"
  bottom: "conv_vally_c2"
  top: "conv_vally_c2"
}

layer {
  name: "conv_vally_c3"
  type: "Convolution"
  bottom: "conv_vally_c2"
  top: "conv_vally_c3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 256
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_vally_c3"
  type: "BatchNorm"
  bottom: "conv_vally_c3"
  top: "conv_vally_c3"
}

layer {
  name: "add_vally_c"
  type: "Eltwise"
  bottom: "conv_vally_b3"
  bottom: "conv_vally_c3"
  top: "conv_vally_c3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_add_vally_c"
  type: "PReLU"
  bottom: "conv_vally_c3"
  top: "conv_vally_c3"
}

# depooling

layer {
  name: "deconv_depooling_i16c128o32"
  type: "Deconvolution"
  bottom: "conv_vally_c3"
  top: "deconv_depooling_i16c128o32"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 2
    pad: 0
    stride: 2
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

# output

layer {
  name: "concat_output_32"
  type: "Concat"
  bottom: "conv_input_32_b3"
  bottom: "deconv_depooling_i16c128o32"
  top: "concat_output_32"
  concat_param { axis: 1 }
}

layer {
  name: "conv_concat_i32c128o32"
  type: "Convolution"
  bottom: "concat_output_32"
  top: "conv_concat_i32c128o32"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

# 32 - stage a

layer {
  name: "conv_output_32_a1"
  type: "Convolution"
  bottom: "conv_concat_i32c128o32"
  top: "conv_output_32_a1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 64
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_32_a1"
  type: "BatchNorm"
  bottom: "conv_output_32_a1"
  top: "conv_output_32_a1"
}

layer {
  name: "relu_output_32_a1"
  type: "PReLU"
  bottom: "conv_output_32_a1"
  top: "conv_output_32_a1"
}

layer {
  name: "conv_output_32_a2"
  type: "Convolution"
  bottom: "conv_output_32_a1"
  top: "conv_output_32_a2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 64
    kernel_size: 3
    pad: 1
    stride: 1
    group: 16
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_32_a2"
  type: "BatchNorm"
  bottom: "conv_output_32_a2"
  top: "conv_output_32_a2"
}

layer {
  name: "relu_output_32_a2"
  type: "PReLU"
  bottom: "conv_output_32_a2"
  top: "conv_output_32_a2"
}

layer {
  name: "conv_output_32_a3"
  type: "Convolution"
  bottom: "conv_output_32_a2"
  top: "conv_output_32_a3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_32_a3"
  type: "BatchNorm"
  bottom: "conv_output_32_a3"
  top: "conv_output_32_a3"
}

layer {
  name: "add_output_32_a"
  type: "Eltwise"
  bottom: "conv_concat_i32c128o32"
  bottom: "conv_output_32_a3"
  top: "conv_output_32_a3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_add_output_32_a"
  type: "PReLU"
  bottom: "conv_output_32_a3"
  top: "conv_output_32_a3"
}

# 32 - stage b

layer {
  name: "conv_output_32_b1"
  type: "Convolution"
  bottom: "conv_output_32_a3"
  top: "conv_output_32_b1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 64
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_32_b1"
  type: "BatchNorm"
  bottom: "conv_output_32_b1"
  top: "conv_output_32_b1"
}

layer {
  name: "relu_output_32_b1"
  type: "PReLU"
  bottom: "conv_output_32_b1"
  top: "conv_output_32_b1"
}

layer {
  name: "conv_output_32_b2"
  type: "Convolution"
  bottom: "conv_output_32_b1"
  top: "conv_output_32_b2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 64
    kernel_size: 3
    pad: 1
    stride: 1
    group: 16
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_32_b2"
  type: "BatchNorm"
  bottom: "conv_output_32_b2"
  top: "conv_output_32_b2"
}

layer {
  name: "relu_output_32_b2"
  type: "PReLU"
  bottom: "conv_output_32_b2"
  top: "conv_output_32_b2"
}

layer {
  name: "conv_output_32_b3"
  type: "Convolution"
  bottom: "conv_output_32_b2"
  top: "conv_output_32_b3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 128
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_32_b3"
  type: "BatchNorm"
  bottom: "conv_output_32_b3"
  top: "conv_output_32_b3"
}

layer {
  name: "add_output_32_b"
  type: "Eltwise"
  bottom: "conv_concat_i32c128o32"
  bottom: "conv_output_32_b3"
  top: "conv_output_32_b3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_bdd_output_32_b"
  type: "PReLU"
  bottom: "conv_output_32_b3"
  top: "conv_output_32_b3"
}

# depooling

layer {
  name: "deconv_depooling_i32c16o64"
  type: "Deconvolution"
  bottom: "conv_output_32_b3"
  top: "deconv_depooling_i32c16o64"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 16
    kernel_size: 2
    pad: 0
    stride: 2
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "concat_output"
  type: "Concat"
  bottom: "conv_input_64_a3"
  bottom: "deconv_depooling_i32c16o64"
  top: "concat_output_64"
  concat_param { axis: 1 }
}

layer {
  name: "conv_concat_i64c16o64"
  type: "Convolution"
  bottom: "concat_output_64"
  top: "conv_concat_i64c16o64"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 16
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "conv_output_64_a1"
  type: "Convolution"
  bottom: "conv_concat_i64c16o64"
  top: "conv_output_64_a1"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 8
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_64_a1"
  type: "BatchNorm"
  bottom: "conv_output_64_a1"
  top: "conv_output_64_a1"
}

layer {
  name: "relu_output_64_a1"
  type: "PReLU"
  bottom: "conv_output_64_a1"
  top: "conv_output_64_a1"
}

layer {
  name: "conv_output_64_a2"
  type: "Convolution"
  bottom: "conv_output_64_a1"
  top: "conv_output_64_a2"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 8
    kernel_size: 3
    pad: 1
    stride: 1
    group: 2
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_64_a2"
  type: "BatchNorm"
  bottom: "conv_output_64_a2"
  top: "conv_output_64_a2"
}

layer {
  name: "relu_output_64_a2"
  type: "PReLU"
  bottom: "conv_output_64_a2"
  top: "conv_output_64_a2"
}

layer {
  name: "conv_output_64_a3"
  type: "Convolution"
  bottom: "conv_output_64_a2"
  top: "conv_output_64_a3"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 16
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "bn_output_64_a3"
  type: "BatchNorm"
  bottom: "conv_output_64_a3"
  top: "conv_output_64_a3"
}

layer {
  name: "add_output_64_a"
  type: "Eltwise"
  bottom: "conv_concat_i64c16o64"
  bottom: "conv_output_64_a3"
  top: "conv_output_64_a3"
  eltwise_param { operation: SUM }
}

layer {
  name: "relu_add_output_64_a"
  type: "PReLU"
  bottom: "conv_output_64_a3"
  top: "conv_output_64_a3"
}

layer {
  name: "crop_output"
  type: "Crop"
  bottom: "conv_output_64_a3"
  bottom: "label"
  top: "crop_output"
  crop_param: {
    offset: 16
  }
}

# 1 * 1 * 1 conv

layer {
  name: "conv_i32c2o32_output_4"
  type: "Convolution"
  bottom: "crop_output"
  top: "conv_i32c2o32_output_4"
  param { lr_mult: 1.0 decay_mult: 1.0 }
  param { lr_mult: 2.0 decay_mult: 0.0 }
  convolution_param {
    engine: CUDNN
    num_output: 2
    kernel_size: 1
    pad: 0
    stride: 1
    weight_filler { type: "msra" variance_norm: 2 }
    bias_filler { type: "constant" value: 0.0 }
  }
}

layer {
  name: "relu_output"
  type: "ReLU"
  bottom: "conv_i32c2o32_output_4"
  top: "conv_i32c2o32_output_4"
}

layer {
  name: "reshape_output"
  type: "Reshape"
  bottom: "conv_i32c2o32_output_4"
  top: "reshape_output"
  reshape_param {
    shape {
      dim: 0  # copy the dimension from below
      dim: 2
      dim: -1 # this should be 32768
    }
  }
}

layer {
  name: "reshape_label"
  type: "Reshape"
  bottom: "label"
  top: "reshape_label"
  reshape_param {
    shape {
      dim: 0  # copy the dimension from below
      dim: 1
      dim: -1 # this should be 32768
    }
  }
}

layer {
  name: "softmax_output"
  type: "Softmax"
  bottom: "reshape_output"
  top: "softmax_output"
}

layer {
  type: "DiceLoss"
  name: "loss"
  top: "loss"
  bottom: "softmax_output"
  bottom: "reshape_label"
}
